# -*- coding: utf-8 -*-
"""scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zGZ6K5K2xgS4yq96UqhHE3hOHEUjdzYd
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

BASE_URL = "https://www.shl.com/solutions/products/product-catalog/"
HEADERS = {"User-Agent": "Mozilla/5.0"}

def extract_details(soup):
    lang, level, length, desc = [], "N/A", "N/A", "N/A"
    lang_found = False

    for row in soup.select(".product-catalogue-training-calendar__row"):
        h, p = row.find("h4"), row.find("p")
        if not h or not p:
            continue
        ht, pt = h.text.strip().lower(), p.text.strip()
        if "language" in ht:
            lang = [pt.strip()]
            lang_found = True
        elif "job level" in ht or "level" in ht:
            level = pt
        elif "assessment length" in ht:
            length = pt
        elif "description" in ht:
            desc = pt

    # Only use the backup language field if not found earlier
    if not lang_found:
        extra = soup.find("p", class_="product-catalogue__download-language")
        if extra:
            lang = [extra.text.strip()]

    lang = [l.strip() for l in lang if l.strip()]
    unique_lang = sorted(set(lang))  # optional sorting
    return ", ".join(unique_lang), level, length, desc

def fetch_detail(a):
    try:
        r = requests.get(a["url"], headers=HEADERS)
        if r.ok:
            soup = BeautifulSoup(r.content, "html.parser")
            a["language"], a["level"], a["length"], a["description"] = extract_details(soup)
    except Exception as e:
        print(f"‚ùå {a['url']}: {e}")
    return a

def parse_table(table):
    data = []
    for row in table.select("tr")[1:]:
        tds = row.find_all("td")
        if len(tds) < 4: continue
        a = tds[0].find("a")
        name, url = (a.text.strip(), "https://www.shl.com" + a["href"]) if a else ("Unknown", "")
        test_type = ", ".join(s.text.strip() for s in tds[3].select(".product-catalogue__key")) or "N/A"
        data.append({
            "name": name, "url": url, "test_type": test_type,
            "remote_testing": "Yes" if tds[1].select_one(".-yes") else "No",
            "adaptive_irt": "Yes" if tds[2].select_one(".-yes") else "No",
            "language": "", "level": "N/A", "length": "N/A", "description": "N/A"
        })
    return data

def scrape_type(type_id, pages, label):
    results = []
    for start in range(0, pages * 12, 12):
        url = f"{BASE_URL}?start={start}&type={type_id}"
        r = requests.get(url, headers=HEADERS)
        if not r.ok: break
        soup = BeautifulSoup(r.content, "html.parser")
        print(f"[{label}] Scraping: {url}")
        table = soup.find("table")
        if not table: break
        chunk = parse_table(table)
        if not chunk: break
        results.extend(chunk)
        time.sleep(1)
    return results

def scrape_catalog():
    print("üîç Fetching SHL Assessments...")
    data = scrape_type(2, 12, "Prepackaged") + scrape_type(1, 32, "Individual")
    print("üì• Fetching Details...")
    with ThreadPoolExecutor(max_workers=5) as ex:
        tasks = {ex.submit(fetch_detail, a): a for a in data}
        for i, _ in enumerate(as_completed(tasks), 1):
            if i % 10 == 0: print(f"Progress: {i}/{len(data)} done")
    return pd.DataFrame(data)

if __name__ == "__main__":
    print("üöÄ Starting Scraper...")
    df = scrape_catalog()
    if not df.empty:
        df.to_csv("shl_assessments.csv", index=False)
        print(f"‚úÖ Saved {len(df)} rows to shl_assessments.csv")
    else:
        print("‚ùå No data scraped.")